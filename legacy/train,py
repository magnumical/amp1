import os
import logging
import gc
from joblib import Parallel, delayed
import mlflow
import mlflow.keras
import numpy as np
import pandas as pd
import librosa
import librosa.display
import optuna
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.utils import to_categorical, normalize
from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D

from imblearn.over_sampling import RandomOverSampler
from keras.preprocessing.image import ImageDataGenerator
from imblearn.over_sampling import SMOTE



from tensorflow.keras.models import Sequential, Model, load_model

from tensorflow.keras.layers import Conv1D, Conv2D, SeparableConv1D, MaxPooling1D, MaxPooling2D
from tensorflow.keras.layers import Input, add, Flatten, Dense, BatchNormalization, Dropout, LSTM, GRU
from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalMaxPooling2D, Activation, LeakyReLU, ReLU

from tensorflow.keras import regularizers
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef
from sklearn.metrics import cohen_kappa_score,roc_auc_score,confusion_matrix,classification_report


# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
data_logger = logging.getLogger("data_loading")
processing_logger = logging.getLogger("data_processing")
model_logger = logging.getLogger("model_training")

# Utility Functions
def load_data():
    """Load patient diagnosis and demographic data."""
    data_logger.info("Loading patient diagnosis and demographic data.")
    diagnosis_df = pd.read_csv('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv', 
                               names=['Patient number', 'Diagnosis'])

    patient_df = pd.read_csv('/kaggle/input/respiratory-sound-database/demographic_info.txt', 
                             names=['Patient number', 'Age', 'Sex', 'Adult BMI (kg/m2)', 'Child Weight (kg)', 'Child Height (cm)'],
                             delimiter=' ')

    data_logger.info("Data successfully loaded.")
    return pd.merge(left=patient_df, right=diagnosis_df, how='left')


def process_audio_file(soundDir, audio_files_path, df_filtered):
    """
    Process a single audio file: extract MFCC features and augment with noise, stretching, and shifting.
    
    Args:
        soundDir: Filename of the audio file.
        audio_files_path: Path to the directory containing audio files.
        df_filtered: Filtered DataFrame containing patient diagnosis and metadata.
        
    Returns:
        Tuple containing features (X_local) and labels (y_local).
    """
    X_local = []
    y_local = []
    features = 52

    # Extract patient ID and disease from filename and DataFrame
    patient_id = int(soundDir.split('_')[0])
    disease = df_filtered.loc[df_filtered['Patient number'] == patient_id, 'Diagnosis'].values[0]

    # Load audio file
    data_x, sampling_rate = librosa.load(os.path.join(audio_files_path, soundDir), sr=None)
    mfccs = np.mean(librosa.feature.mfcc(y=data_x, sr=sampling_rate, n_mfcc=features).T, axis=0)
    X_local.append(mfccs)
    y_local.append(disease)

    # Data augmentation
    for augmentation in [add_noise, shift, stretch, pitch_shift]:
        if augmentation == add_noise:
            augmented_data = augmentation(data_x, 0.001)
        elif augmentation == shift:
            augmented_data = augmentation(data_x, 1600)
        elif augmentation == stretch:
            augmented_data = augmentation(data_x, 1.2)
        elif augmentation == pitch_shift:
            augmented_data = augmentation(data_x, 3)

        mfccs_augmented = np.mean(librosa.feature.mfcc(y=augmented_data, sr=sampling_rate, n_mfcc=features).T, axis=0)
        X_local.append(mfccs_augmented)
        y_local.append(disease)

    return X_local, y_local



def mfccs_feature_exteraction(audio_files_path, df_filtered, n_jobs=-1):
    """
    Extract MFCC features from audio data and augment with noise, stretching, and shifting in parallel.
    
    Args:
        audio_files_path: Path to the directory containing audio files.
        df_filtered: Filtered DataFrame containing patient diagnosis and metadata.
        n_jobs: Number of parallel jobs (-1 to use all available cores).

    Returns:
        X_data: Array of features extracted from the audio files.
        y_data: Array of target labels.
    """
    processing_logger.info(f"Processing audio files in: {audio_files_path}")
    files = [file for file in os.listdir(audio_files_path) if file.endswith('.wav') and file[:3] not in ['103', '108', '115']]
   
    #files = files[:40] ## DEBUG

    # Use Parallel and delayed to process files in parallel
    results = Parallel(n_jobs=n_jobs, backend="loky")(delayed(process_audio_file)(file, audio_files_path, df_filtered) for file in tqdm(files, desc="Processing audio files"))

    # Flatten results
    X_ = []
    y_ = []
    for X_local, y_local in results:
        X_.extend(X_local)
        y_.extend(y_local)

    X_data = np.array(X_)
    y_data = np.array(y_)
    processing_logger.info("MFCC feature extraction and augmentation complete.")
    return X_data, y_data



def add_noise(data,x):
    noise = np.random.randn(len(data))
    data_noise = data + x * noise
    return data_noise

def shift(data, x):
    return np.roll(data, int(x))

def stretch(data, rate):
    """Apply time-stretching to the audio signal."""
    return librosa.effects.time_stretch(data, rate=rate)



def pitch_shift (data , rate):
    data = librosa.effects.pitch_shift(data, sr=220250, n_steps=rate)
    return data

def prepare_dataset_with_gru(df_filtered, audio_files_path):
    """Prepare the dataset using the GRU pipeline."""
    processing_logger.info("Preparing dataset with GRU pipeline.")
    X, y = mfccs_feature_exteraction(audio_files_path, df_filtered)  # Pass filtered DataFrame
    le = LabelEncoder()
    y = to_categorical(le.fit_transform(np.array(y)))
    processing_logger.info("Dataset preparation with GRU pipeline complete.")
    return X, y, le



def process_audio_metadata(folder_path):
    """Extract audio metadata from filenames."""
    processing_logger.info("Extracting audio metadata from filenames.")
    data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            parts = filename.split('_')
            data.append({
                'Patient number': int(parts[0]),
                'Recording index': parts[1],
                'Chest location': parts[2],
                'Acquisition mode': parts[3],
                'Recording equipment': parts[4].split('.')[0]
            })
    processing_logger.info("Audio metadata extraction complete.")
    return pd.DataFrame(data)

def merge_datasets(df1, df2):
    """Merge metadata and diagnosis data."""
    processing_logger.info("Merging metadata and diagnosis data.")
    merged_df = pd.merge(left=df1, right=df2, how='left').sort_values('Patient number').reset_index(drop=True)
    merged_df['audio_file_name'] = merged_df.apply(lambda row: f"{row['Patient number']}_{row['Recording index']}_{row['Chest location']}_{row['Acquisition mode']}_{row['Recording equipment']}.wav", axis=1)
    processing_logger.info("Merging complete.")
    return merged_df

def filter_and_sample_data(df, mode='binary'):
    """
    Filter and sample the dataset.
    
    Args:
        df: Input DataFrame containing diagnosis data.
        mode: Specify 'binary' for Normal/Abnormal or 'multiclass' for original labels.
        
    Returns:
        Filtered and processed DataFrame.
    """
    processing_logger.info("Filtering and sampling the dataset.")
    if mode == 'binary':
        df['Diagnosis'] = df['Diagnosis'].apply(lambda x: 'Abnormal' if x != 'Healthy' else 'Normal')
    # Else, keep original multiclass labels
    df = df.sort_values('Patient number').reset_index(drop=True)
    processing_logger.info(f"Filtering and sampling complete with mode={mode}.")
    return df


def oversample_data(X, y):
    """Apply SMOTE to balance classes."""
    processing_logger.info("Applying SMOTE to balance classes.")
    
    # Save the original shape of features
    original_shape = X.shape[1:]  
    
    # Flatten for SMOTE processing
    X = X.reshape((X.shape[0], -1))
    
    # Convert one-hot encoded labels to integers
    y = np.argmax(y, axis=1)
    
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    
    # Reshape back to the original dimensions
    X_resampled = X_resampled.reshape((-1, *original_shape))
    
    # Convert labels back to one-hot encoding
    y_resampled = to_categorical(y_resampled)
    
    processing_logger.info("SMOTE oversampling complete.")
    return X_resampled, y_resampled



def augment_data(X, y):
    """Apply data augmentation to increase dataset size."""
    processing_logger.info("Applying data augmentation.")
    datagen = ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        horizontal_flip=True
    )
    datagen.fit(X)
    processing_logger.info("Data augmentation setup complete.")
    return datagen

def preprocess_file(row, audio_files_path, mode):
    """Preprocess a single audio file."""
    file_path = os.path.join(audio_files_path, row['audio_file_name'])
    feature = preprocessing(file_path, mode)
    label = row['Diagnosis']
    return feature, label

def prepare_dataset_parallel(df, audio_files_path, mode):
    """Prepare the dataset by extracting features from audio files in parallel."""
    processing_logger.info(f"Preparing dataset using {mode} features in parallel.")
    results = Parallel(n_jobs=-1)(delayed(preprocess_file)(row, audio_files_path, mode) for _, row in tqdm(df.iterrows(), total=len(df)))

    X, y = zip(*results)
    X = np.array(X)
    X = np.expand_dims(X, axis=-1)  # Add channel dimension
    X = normalize(X, axis=1)
    le = LabelEncoder()
    y = to_categorical(le.fit_transform(np.array(y)))

    processing_logger.info(f"Dataset preparation using {mode} complete.")
    return X, y, le

def preprocessing(audio_file, mode):
    """Preprocess audio file by resampling, padding/truncating, and extracting features."""
    sr_new = 16000  # Resample audio to 16 kHz
    x, sr = librosa.load(audio_file, sr=sr_new)

    # Padding or truncating to 5 seconds (5 * sr_new samples)
    max_len = 5 * sr_new
    if x.shape[0] < max_len:
        x = np.pad(x, (0, max_len - x.shape[0]))
    else:
        x = x[:max_len]

    # Extract features
    if mode == 'mfcc':
        feature = librosa.feature.mfcc(y=x, sr=sr_new, n_mfcc=20)  # Ensure consistent shape
    elif mode == 'log_mel':
        feature = librosa.feature.melspectrogram(y=x, sr=sr_new, n_mels=20, fmax=8000)  # Match n_mels to 20
        feature = librosa.power_to_db(feature, ref=np.max)

    return feature

def prepare_dataset(df, audio_files_path, mode):
    """Prepare the dataset by extracting features from audio files."""
    processing_logger.info(f"Preparing dataset using {mode} features.")
    X, y = [], []
    for _, row in tqdm(df.iterrows(), total=len(df)):
        file_path = os.path.join(audio_files_path, row['audio_file_name'])
        feature = preprocessing(file_path, mode)
        X.append(feature)
        y.append(row['Diagnosis'])
        del feature  # Free memory after processing each file
        gc.collect()

    X = np.array(X)
    X = np.expand_dims(X, axis=-1)  # Add channel dimension
    X = normalize(X, axis=1)
    le = LabelEncoder()
    y = to_categorical(le.fit_transform(np.array(y)))
    processing_logger.info(f"Dataset preparation using {mode} complete.")
    return X, y, le

def build_model(input_shape, n_filters, dense_units, dropout_rate):
    """Build and compile the updated CNN model."""
    model_logger.info("Building the updated CNN model.")
    model = Sequential([
        Conv2D(n_filters, (3, 3), activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling2D((2, 2)) if input_shape[0] >= 2 else None,
        Dropout(dropout_rate),

        Conv2D(n_filters * 2, (3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D((2, 2)) if input_shape[0] >= 4 else None,
        Dropout(dropout_rate),

        Conv2D(n_filters * 4, (3, 3), activation='relu'),
        BatchNormalization(),
        # Use GlobalAveragePooling2D to handle small spatial dimensions
        GlobalAveragePooling2D(),
        Dropout(dropout_rate),

        Dense(dense_units, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model_logger.info("Model built and compiled successfully.")
    return model


def log_metrics(y_true, y_pred, mode):
    """Log evaluation metrics."""
    precision = classification_report(y_true, y_pred, output_dict=True)['weighted avg']['precision']
    recall = classification_report(y_true, y_pred, output_dict=True)['weighted avg']['recall']
    f1_score = classification_report(y_true, y_pred, output_dict=True)['weighted avg']['f1-score']

    mlflow.log_metric(f"{mode}_precision", precision)
    mlflow.log_metric(f"{mode}_recall", recall)
    mlflow.log_metric(f"{mode}_f1_score", f1_score)

def evaluate_model(model, X_test, y_test, le, mode):
    """Evaluate the model and display results."""
    model_logger.info(f"Evaluating the model using {mode} features.")
    predictions = model.predict(X_test)
    predicted_classes = np.argmax(predictions, axis=1)
    y_true = np.argmax(y_test, axis=1)

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_true, predicted_classes)
    plt.figure(figsize=(8, 6))
    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix ({mode})")
    plt.colorbar()
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    cm_path = f"confusion_matrix_{mode}.png"
    plt.savefig(cm_path)
    mlflow.log_artifact(cm_path)

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_true, predictions[:, 1])
    auc_score = roc_auc_score(y_true, predictions[:, 1])
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC curve (area = {auc_score:.2f})")
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f"Receiver Operating Characteristic ({mode})")
    plt.legend(loc="lower right")
    roc_path = f"roc_curve_{mode}.png"
    plt.savefig(roc_path)
    mlflow.log_artifact(roc_path)

    # Log metrics
    mlflow.log_metric(f"{mode}_auc", auc_score)
    log_metrics(y_true, predicted_classes, mode)

    model_logger.info(f"Model evaluation using {mode} features complete.")


def track_experiment_with_mlflow_and_optuna(mode):
    """Optimize hyperparameters using Optuna and track experiments with MLflow."""
    def objective(trial):
        with mlflow.start_run(nested=True):  # Start a new MLflow run for each trial
            # Hyperparameters to tune
            n_filters = trial.suggest_categorical('n_filters', [16, 32])
            dense_units = trial.suggest_int('dense_units', 64, 128, step=32)
            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3, step=0.1)
            learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)

            # Build and compile the model
            model = build_model(X_train.shape[1:], n_filters, dense_units, dropout_rate)
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

            # Train the model
            history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)

            # Log hyperparameters and metrics to MLflow
            mlflow.log_params({
                'n_filters': n_filters,
                'dense_units': dense_units,
                'dropout_rate': dropout_rate,
                'learning_rate': learning_rate
            })
            mlflow.log_metric("best_val_accuracy", max(history.history['val_accuracy']))

            # Save training and validation loss curves
            plt.figure()
            plt.plot(history.history['loss'], label='Train Loss')
            plt.plot(history.history['val_loss'], label='Validation Loss')
            plt.legend()
            plt.title("Training and Validation Loss")
            loss_curve_path = f"loss_curve_{trial.number}.png"
            plt.savefig(loss_curve_path)
            mlflow.log_artifact(loss_curve_path)

            return max(history.history['val_accuracy'])

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=20)

    best_trial = study.best_trial
    model_logger.info(f"Best Trial for {mode}: {best_trial.params}")

    best_model = build_model(X_train.shape[1:], best_trial.params['n_filters'], best_trial.params['dense_units'], best_trial.params['dropout_rate'])
    best_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=1)
    best_model.save(f"/kaggle/working/best_model_{mode}.h5")
    mlflow.log_artifact(f"/kaggle/working/best_model_{mode}.h5")
    model_logger.info(f"Best model for {mode} saved successfully.")
    return best_model

def optimize_gru_model(trial):
    """
    Define the GRU model and its hyperparameter optimization using Optuna.
    """
    num_units = trial.suggest_int("num_units", 64, 256, step=32)
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5, step=0.1)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-4, 1e-2)
    
    # Build the model
    model = build_gru_model(
        input_shape=X_train.shape[1:],  # Use global variables from main
        num_classes=y_train.shape[1]
    )
    model.compile(
        optimizer=Adamax(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    # Fit the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=10,
        batch_size=32,
        verbose=0
    )
    
    # Return validation accuracy
    val_accuracy = max(history.history["val_accuracy"])
    return val_accuracy

def optimize_gru_model(trial):
    """
    Define the GRU model and its hyperparameter optimization using Optuna.
    """
    num_units = trial.suggest_int("num_units", 64, 256, step=32)
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5, step=0.1)
    learning_rate = trial.suggest_loguniform("learning_rate", 1e-4, 1e-2)
    
    # Build the model
    model = build_gru_model(
        input_shape=X_train.shape[1:],  # Use global variables from main
        num_classes=y_train.shape[1]
    )
    model.compile(
        optimizer=Adamax(learning_rate=learning_rate),
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    
    # Fit the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=10,
        batch_size=32,
        verbose=0
    )
    
    # Return validation accuracy
    val_accuracy = max(history.history["val_accuracy"])
    return val_accuracy

def build_gru_model(input_shape, num_classes):
    """
    Build a GRU model with the specified architecture.

    Args:
        input_shape: Shape of the input data (e.g., (1, 52)).
        num_classes: Number of output classes.

    Returns:
        Compiled GRU model.
    """
    # Input layer
    input_layer = Input(shape=input_shape)

    # First Convolutional Block
    conv_block1 = Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)
    conv_block1 = MaxPooling1D(pool_size=2, strides=2, padding='same')(conv_block1)
    conv_block1 = BatchNormalization()(conv_block1)

    # Second Convolutional Block
    conv_block2 = Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu')(conv_block1)
    conv_block2 = MaxPooling1D(pool_size=2, strides=2, padding='same')(conv_block2)
    conv_block2 = BatchNormalization()(conv_block2)

    # GRU Branch 1
    gru_branch1 = GRU(32, return_sequences=True, activation='tanh', go_backwards=True)(conv_block2)
    gru_branch1 = GRU(128, return_sequences=True, activation='tanh', go_backwards=True)(gru_branch1)

    # GRU Branch 2
    gru_branch2 = GRU(64, return_sequences=True, activation='tanh', go_backwards=True)(conv_block2)
    gru_branch2 = GRU(128, return_sequences=True, activation='tanh', go_backwards=True)(gru_branch2)

    # GRU Branch 3
    gru_branch3 = GRU(64, return_sequences=True, activation='tanh', go_backwards=True)(conv_block2)
    gru_branch3 = GRU(128, return_sequences=True, activation='tanh', go_backwards=True)(gru_branch3)

    # Combine GRU Branches
    combined_gru_branches = add([gru_branch1, gru_branch2, gru_branch3])

    # Additional GRU Layers
    additional_gru1 = GRU(128, return_sequences=True, activation='tanh', go_backwards=True)(combined_gru_branches)
    additional_gru1 = GRU(32, return_sequences=False, activation='tanh', go_backwards=True)(additional_gru1)

    # Fully Connected Layers
    dense_block = Dense(64, activation=None)(additional_gru1)
    dense_block = LeakyReLU()(dense_block)
    dense_block = Dense(32, activation=None)(dense_block)
    dense_block = LeakyReLU()(dense_block)

    # Output Layer
    output_layer = Dense(num_classes, activation="softmax")(dense_block)

    # Build and Compile Model
    gru_model = Model(inputs=input_layer, outputs=output_layer)
    gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return gru_model

def main():
    data_logger.info("Starting data pipeline.")
    df = load_data()
    audio_metadata = process_audio_metadata('/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files')
    df_all = merge_datasets(audio_metadata, df)
    df_filtered = filter_and_sample_data(df_all)

    models = []
    modes = ['gru', 'mfcc', 'log_mel']  # Add 'gru' for multi-class, others for binary classification
    for mode in modes:
        global X_train, X_val, X_test, y_train, y_val, y_test
        if mode == 'gru':
            X, y, le = prepare_dataset_with_gru(df_filtered, '/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files')
        else:
            X, y, le = prepare_dataset_parallel(df_filtered, '/kaggle/input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files', mode=mode)

        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

        # Check class distribution
        unique_classes, class_counts = np.unique(np.argmax(y_train, axis=1), return_counts=True)
        processing_logger.info(f"Classes in y_train: {unique_classes}, Counts: {class_counts}")
        if len(unique_classes) <= 1:
            raise ValueError(f"Insufficient class diversity in y_train for {mode} mode.")

        # Apply oversampling only for multi-class tasks
        if mode == 'gru':
            try:
                X_train, y_train = oversample_data(X_train, y_train)
            except ValueError as e:
                processing_logger.warning(f"SMOTE skipped: {e}")
        else:
            processing_logger.info("Oversampling skipped for binary classification.")

        model_logger.info(f"Running experiment for {mode} features.")
        with mlflow.start_run(run_name=f"Experiment_{mode}"):
            if mode == 'gru':
                # Expand dimensions for GRU input
                X_train = np.expand_dims(X_train, axis=1)
                X_val = np.expand_dims(X_val, axis=1)
                X_test = np.expand_dims(X_test, axis=1)
                
                # Perform optimization with Optuna
                study = optuna.create_study(direction="maximize")
                study.optimize(optimize_gru_model, n_trials=20)
                
                # Log the best hyperparameters
                best_params = study.best_params
                model_logger.info(f"Best GRU Hyperparameters: {best_params}")
                mlflow.log_params(best_params)
                
                # Build and save the best model
                best_model = build_gru_model(
                    input_shape=X_train.shape[1:],
                    num_classes=y_train.shape[1]
                )
                best_model.compile(
                    optimizer=Adamax(learning_rate=best_params["learning_rate"]),
                    loss="categorical_crossentropy",
                    metrics=["accuracy"]
                )
                best_model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=10,
                    batch_size=32,
                    verbose=1
                )
                
                # Save the best model
                checkpoint_path = f"/kaggle/working/best_gru_model_{mode}.keras"
                best_model.save(checkpoint_path, save_format="keras")
                mlflow.log_artifact(checkpoint_path)
                models.append(best_model)

            else:
                model = track_experiment_with_mlflow_and_optuna(mode)
                models.append(model)


if __name__ == "__main__":
    main()
